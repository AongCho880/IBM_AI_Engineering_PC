# üìò Decision Trees in Machine Learning ‚Äî Short Notes with Figures

## 1. What is a Decision Tree?

A **Decision Tree** is a supervised machine learning algorithm used mainly for **classification**.

It works like a **flowchart** that makes decisions step by step.

### Main Parts:

* **Internal Node** ‚Üí Tests a feature
* **Branch** ‚Üí Result of the test
* **Leaf Node** ‚Üí Final class/output

üëâ It answers questions like:
‚ÄúIs age > 40?‚Äù ‚Üí ‚ÄúIs cholesterol high?‚Äù ‚Üí ‚ÄúWhich drug?‚Äù


## üìä Structure of a Decision Tree

![Image](https://images.openai.com/static-rsc-3/4I3x11hOXIvss3gkDFVqChfolOGVph1-N96UfzBU0Oy0P_XVfQVSoex6TMmXGxZ_DZJALr3WFW26oTcqz8nIoOjQ1EquVAy1hGDlVt-AbAQ?purpose=fullsize\&v=1)

![Image](https://www.researchgate.net/publication/272666514/figure/fig1/AS%3A612917321953280%401523142285587/Flowchart-of-C45-decision-tree-algorithm.png)

![Image](https://www.researchgate.net/publication/303773171/figure/fig2/AS%3A391407152975874%401470330145270/a-describes-the-components-of-a-decision-tree-the-Nodes-represent-the-possible.png)

![Image](https://www.researchgate.net/publication/382914996/figure/fig2/AS%3A11431281270029901%401723000998849/Structure-of-Decision-Tree-Nodes-Root-Interior-and-Leaf.ppm)

![Image](https://wiki.pathmind.com/images/wiki/decision_tree_nodes.png)

### Example:

For medical data:

* Root: Age
* Next: Gender / Cholesterol
* Leaf: Drug A or Drug B

Each path from top to bottom is a **decision rule**.


## 2. Example: Drug Prediction

### Problem:

Predict whether a patient needs **Drug A or Drug B**.

### Input Features:

* Age
* Gender
* Blood Pressure
* Cholesterol

### Output:

* Drug A
* Drug B

### Working:

The tree checks patient information and follows branches until it reaches a decision.

üëâ Decision is based on past patient data.


## 3. How is a Decision Tree Built?

Decision Trees are built using **recursive splitting**.

### Steps:

1. Start with all training data
2. Select the best feature
3. Split data into groups
4. Create new nodes
5. Repeat
6. Stop when conditions are met

This process is called **recursive partitioning**.


## üå≥ Building a Decision Tree (Training Process)

![Image](https://miro.medium.com/0%2Apb-1ufHK-OmR8k7r.png)

![Image](https://images.openai.com/static-rsc-3/1dkR9kvb99D09fHyKjL7H2YFbnX6gAHa1O_9e_52ynR4nHgns1FIDnHwiAkBycIOlaAOEEePpkUmhs18uyDUCZZ5uWcpS0faRuoVdtfQCc0?purpose=fullsize\&v=1)

![Image](https://www.researchgate.net/publication/339716153/figure/fig1/AS%3A865707545878528%401583412171698/Example-of-a-calculated-recursive-partition-decision-tree-Algorithm-Builder-v18.jpg)

![Image](https://www.researchgate.net/publication/51979650/figure/fig3/AS%3A195881040125952%401423713087012/Decision-tree-constructed-by-recursive-partitioning-analysis-test-sample-The-plots-of.png)

![Image](https://www.researchgate.net/publication/330227554/figure/fig5/AS%3A960339940229149%401605974292484/Example-of-decision-tree-recursively-partitioned-predictor-space-left-and-the.png)

### Key Idea:

At every step, the algorithm chooses the **best feature** to divide the data.

Goal: Make each group as **pure** as possible.


## 4. How Does the Tree Choose the Best Feature?

To split data, the algorithm uses **split measures**.

### Common Measures:

### ‚úÖ Entropy

* Measures **randomness**
* High entropy ‚Üí Mixed data
* Low entropy ‚Üí Pure data

Range:

* 0 ‚Üí Fully pure
* 1 ‚Üí Fully mixed


### ‚úÖ Information Gain

Shows how much **uncertainty is reduced** after splitting.

Formula (concept):

> Information Gain = Entropy (Before) ‚àí Entropy (After)

üëâ Higher gain = Better feature


### ‚úÖ Gini Impurity

* Measures wrong classification
* Lower value = Better split

(Libraries calculate this automatically)


## üìà Entropy and Information Gain Visualization

![Image](https://miro.medium.com/1%2AKKICWqYsVwac-SnP1Mt4Gg.png)

![Image](https://ekamperi.github.io/images/decision_trees/gini_vs_entropy.png)

![Image](https://miro.medium.com/1%2AOctuEVmm38Xeo-jfCHPRdw.png)

![Image](https://miro.medium.com/0%2AavdB7itYtq-oKIU7.png)

### Goal of Splitting:

‚úîÔ∏è Reduce entropy
‚úîÔ∏è Increase information gain
‚úîÔ∏è Make leaves more pure


## 5. When Does the Tree Stop Growing?

A tree stops when:

* Maximum depth is reached
* Minimum samples in a node is reached
* Minimum samples in a leaf is reached
* Maximum number of leaves is reached

This is called:

> ‚úÖ Pre-pruning (Early stopping)


## 6. Pruning in Decision Trees

Sometimes trees become too large and complex.

This leads to:

> ‚ùå Overfitting (memorizing training data)

### What is Pruning?

Removing unnecessary branches.

### Why Prune?

* Reduces overfitting
* Removes noise
* Improves accuracy
* Makes model simpler

### Types:

1. Pre-pruning ‚Üí Stop early
2. Post-pruning ‚Üí Cut after training


## 7. Advantages of Decision Trees

### ‚úÖ Easy to Understand

* Looks like a flowchart

### ‚úÖ Highly Interpretable

* You can see each decision

### ‚úÖ Feature Importance

* Shows which features matter most

### ‚úÖ No Data Scaling Needed

* Works with raw data


## 8. Limitations

### ‚ùå Overfitting

* Large trees memorize data

### ‚ùå Sensitive to Noise

* Small changes affect structure

### ‚ùå Less Stable

* Compared to Random Forests

## 9. Quick Summary (For Revision)

| Topic          | Key Point                    |
| -------------- | ---------------------------- |
| Definition     | Flowchart-based ML algorithm |
| Structure      | Nodes, branches, leaves      |
| Training       | Recursive splitting          |
| Split Measures | Entropy, Gain, Gini          |
| Stopping       | Depth, samples, leaves       |
| Pruning        | Controls overfitting         |
| Advantage      | Simple & visual              |


## 10. Final Conclusion

Decision Trees classify data by asking a sequence of logical questions based on features. They use entropy, information gain, or Gini impurity to choose the best splits. Pruning helps prevent overfitting. Because of their simple and visual nature, Decision Trees are widely used and easy to understand in machine learning.


If you‚Äôd like, I can now prepare:

‚úÖ One-page exam cheat sheet
‚úÖ Numerical example of entropy & gain
‚úÖ Comparison with Random Forest
‚úÖ Practice MCQs
